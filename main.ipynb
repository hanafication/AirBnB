{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1II5YJhleMs5FPUwPVTw92rUHA9S6-D9T","authorship_tag":"ABX9TyN9aLpx4a3xzOIYyWy/+ImR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kz2NOflQrUoM","executionInfo":{"status":"ok","timestamp":1622255931441,"user_tz":-420,"elapsed":11213,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}},"outputId":"d36613f2-ff51-4894-9866-b0e41ff8f8b0"},"source":["!pip install selenium\n","!pip install beautifulsoup4\n","!pip install bs4\n","!pip install requests"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n","Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2pgKP7jlw6G_","executionInfo":{"status":"ok","timestamp":1622255936656,"user_tz":-420,"elapsed":5234,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}},"outputId":"2781489e-38b4-4d73-ab06-f6a729b02f97"},"source":["!apt-get update\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin"],"execution_count":41,"outputs":[{"output_type":"stream","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Co\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [2 InRelease 14.2 kB/88.7\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n","\r0% [1 InRelease gpgv 3,626 B] [Connecting to ppa.launchpad.net (91.189.95.85)] \r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Fetched 252 kB in 2s (114 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","chromium-chromedriver is already the newest version (90.0.4430.93-0ubuntu0.18.04.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 86 not upgraded.\n","cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z5P71f_pxMJM","executionInfo":{"status":"ok","timestamp":1622255936657,"user_tz":-420,"elapsed":21,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}}},"source":["import sys\n","sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1SVm4vRreAZ","executionInfo":{"status":"ok","timestamp":1622255936658,"user_tz":-420,"elapsed":20,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}}},"source":["# Import libraries\n","from selenium import webdriver\n","from bs4 import BeautifulSoup\n","from selenium.webdriver.support import expected_conditions\n","from selenium.webdriver.support.wait import WebDriverWait\n","from selenium.webdriver.common.action_chains import ActionChains\n","from selenium.webdriver.common.keys import Keys\n","from selenium.webdriver.common.by import By\n","import os\n","import time\n","import datetime\n","import pandas as pd\n","import requests"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2BVuqZltxh1","executionInfo":{"status":"ok","timestamp":1622255936659,"user_tz":-420,"elapsed":20,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}}},"source":["def scraper():\n","  # Webdriver setting\n","  chrome_options = webdriver.ChromeOptions()\n","  chrome_options.add_argument('--headless')\n","  chrome_options.add_argument('--no-sandbox')\n","  chrome_options.add_argument('--disable-dev-shm-usage')\n","  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n","\n","  # Get website url\n","  url = 'https://www.airbnb.com/'\n","  driver.get(url)\n","  return driver\n"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XOqCc9PtCoZ","executionInfo":{"status":"ok","timestamp":1622255936660,"user_tz":-420,"elapsed":20,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}}},"source":["class pageScrape:\n","\n","    # All information in elements of listings\n","    LISTING_INFORMATION = {'url': {'tag': 'a', 'get': 'href'},\n","                           'name': {'tag': 'span',  'class' : '_1whrsux9'},\n","                           'superhost': {'tag': 'div', 'class': '_ufoy4t'},\n","                           'desc': {'tag': 'div', 'class': '_b14dlit'},\n","                           'rooms': {'tag': 'div', 'class': '_kqh46o'},\n","                           'facilities': {'tag': 'div', 'class': '_kqh46o', 'order': 1},\n","                           'rating_reviews': {'tag': 'span', 'class': '_18khxk1'},\n","                           'price': {'tag': 'span', 'class': '_155sga30'}\n","                           }\n","    # Constructor\n","    def __init__(self, driver, curr_url):\n","        self.driver = driver\n","        self.curr_url = curr_url\n","\n","\n","    def using_selenium(self):\n","        ''''\n","        Get listings elements from search url\n","        using selenium\n","        '''\n","        # Getting page source/html\n","        #driver = self.driver\n","        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n","        #listings = soup.find_all('div', {'class' : '_8s3ctt'})\n","        # Getting single listings element\n","        #listing = soup.find_all('div', '_gig1e7')\n","        return soup\n","\n","    # Get all elements with target information\n","    def using_requests(self):\n","        ''''\n","        Get listings elements from searched url using requests\n","        '''\n","        soup = BeautifulSoup(requests.get(self.curr_url).content, 'html.parser')\n","        listings = soup.find_all('div', class_ = '_8s3ctt')\n","        print(len(listings))\n","        return listings\n","\n","    def extract_single_listings(self, listings,loc):\n","        '''\n","        Extract all listings in single page\n","        '''\n","\n","        listings_list = list()\n","\n","        for listing in listings:\n","            listings_dict = dict()\n","            listings_dict['city'] = loc\n","            # Iterating for each information\n","            for info in pageScrape.LISTING_INFORMATION:\n","                try:\n","                    if 'class' in pageScrape.LISTING_INFORMATION[info]:\n","                        element = listing.find_all(pageScrape.LISTING_INFORMATION[info]['tag'], pageScrape.LISTING_INFORMATION[info]['class'])\n","                    elif 'span' in pageScrape.LISTING_INFORMATION[info]:\n","                        element = listing.find_all(pageScrape.LISTING_INFORMATION[info]['tag'], pageScrape.LISTING_INFORMATION[info]['span'])\n","                    else:\n","                        element = listing.find_all(pageScrape.LISTING_INFORMATION[info]['tag'])\n","\n","\n","                    # Element\n","                    orders = pageScrape.LISTING_INFORMATION[info].get('order', 0)\n","                    elements = element[orders]\n","\n","                    # Values\n","                    if 'get' in pageScrape.LISTING_INFORMATION[info]:\n","                        output = elements.get(pageScrape.LISTING_INFORMATION[info]['get'])\n","                    else:\n","                        output = elements.get_text()\n","\n","                    listings_dict[info] = output\n","                except:\n","                    listings_dict[info] = 'empty'\n","\n","\n","            listings_list.append(listings_dict)\n","\n","        return listings_list"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTPrqAxytFFQ","executionInfo":{"status":"ok","timestamp":1622257559203,"user_tz":-420,"elapsed":324,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}}},"source":["def search_and_go(loc, driver):\n","    ''''\n","    Melakukan pencarian kota berdasarkan input\n","    loc = input kota\n","    '''\n","    # Menentukan lokasi search box\n","    #search = driver.find_element_by_class_name('_1xq16jy')\n","    #search = driver.find_element_by_xpath(\"//input[@placeholder='Where are you going?']\")\n","    time.sleep(3)\n","    search = WebDriverWait(driver, 10).until(expected_conditions.presence_of_element_located((By.XPATH, \"//input[@placeholder='Where are you going?']\")))\n","    #search = WebDriverWait(driver, 20).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"_1xq16jy\")))\n","\n","    #driver.find_element_by_class_name('_1xq16jy').click()\n","    driver.implicitly_wait(5)\n","    search.send_keys(loc)\n","    driver.implicitly_wait(10)\n","    time.sleep(2)\n","    #actions = ActionChains(driver)\n","    #search.send_keys(Keys.TAB + Keys.TAB + Keys.TAB + Keys.TAB + Keys.TAB + Keys.ENTER)\n","    WebDriverWait(driver, 20).until(expected_conditions.presence_of_element_located((By.CLASS_NAME, \"_1mzhry13\"))).click\n","    # Send keys without element\n","    \n","    #search.send_keys(Keys.RETURN)\n","    #actions.send_keys(Keys.TAB)\n","    #time.sleep(5)\n","    #actions.send_keys(Keys.TAB)\n","    #time.sleep(5)\n","    #actions.send_keys(Keys.TAB)\n","    #time.sleep(5)\n","    #actions.send_keys(Keys.TAB)\n","    #time.sleep(5)\n","    #actions.send_keys(Keys.TAB)\n","    #time.sleep(5)\n","    #actions.send_keys(Keys.RETURN)\n","\n","    go = WebDriverWait(driver, 30).until(expected_conditions.element_to_be_clickable((By.CLASS_NAME, '_1mzhry13'))).click()\n","    curr_url = driver.current_url\n","    return curr_url\n","\n","\n","    # find\n","    #go = driver.find_element_by_class_name('_m9v25n')\n","    #return go.click()\n","\n","def iterating_pages(driver, loc):\n","    ''''\n","    Scrape all listings in all page\n","    '''\n","    scraped_data = list()\n","\n","    while True:\n","        try:\n","            # Process\n","            print('Processing city: {}'.format(loc))\n","            current_page = driver.current_url\n","            #print(current_page)\n","            soup = pageScrape(driver=driver, curr_url=current_page)\n","            page = soup.using_requests()\n","            if len(page) > 0:\n","              page_list = soup.extract_single_listings(listings = page, loc=loc)\n","              scraped_data.append(page_list)\n","\n","              # Move to next page\n","              time.sleep(2)\n","              next_page = WebDriverWait(driver, 5).until(expected_conditions.element_to_be_clickable((By.CLASS_NAME, '_za9j7e')))\n","              next_page.click()\n","            else:\n","              break\n","        except:\n","            #print('Keknya ada yang error')\n","            print('No more page left. Scraping is done')\n","            driver.close()\n","            break\n","\n","    return scraped_data\n"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqv9QnqdtL8C","executionInfo":{"status":"ok","timestamp":1622256890377,"user_tz":-420,"elapsed":336,"user":{"displayName":"Rahadian Ardani Pangestu","photoUrl":"","userId":"10289681423205092581"}}},"source":["def to_pandas(var):\n","    ''''\n","    Convert scraped data to dataframe and save as csv\n","    var = list of scraped data\n","    '''\n","    dataframe = pd.DataFrame()\n","    for dim in var:\n","      df_dic = pd.DataFrame.from_records(dim)\n","      dataframe = dataframe.append(df_dic, ignore_index=True)\n","    dataframe = dataframe.drop_duplicates()\n","    return dataframe\n","\n","def to_csv(loc, dataframe):\n","    '''\n","    Export to csv based on time exporting\n","    '''\n","    path = '/content/drive/MyDrive/Airbnb/exported_dataset'\n","    #path_ubuntu = '/home/expiatio/PycharmProjects/AirBnB/exported_dataset'\n","    ts = time.time()\n","    timenow = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d%H%M%S_%f')\n","\n","    return dataframe.to_csv(os.path.join(path, timenow+'-'+loc+'.csv'),\n","                            index=False, sep=';')"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH0ix2Djtl8a","outputId":"cc69d760-db70-451f-8493-0ffb430d26ad"},"source":["while True:\n","    print('Menu')\n","    print('1. Scrape City')\n","    print('2. Exit\\n')\n","    menu = input('Choose a menu:')\n","\n","    if menu == str(1):\n","      locs = list(map(str, input('Please input city: ').strip().split(',')))\n","      print(locs)\n","      for loc in locs:\n","        print(loc)\n","        driver = scraper()\n","        # search = driver.find_element_by_class_name('_1xq16jy').click()\n","        curr_url = search_and_go(loc=loc, driver=driver)\n","\n","        test_pages = iterating_pages(driver=driver, loc=loc)\n","        test_scrape = to_csv(loc=loc, dataframe=to_pandas(test_pages))\n","    elif menu == str(2):\n","        break\n","    else:\n","        print('I know you wanna go home')\n","        break\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Menu\n","1. Scrape City\n","2. Exit\n","\n","Choose a menu:1\n","Please input city: bangkalan,banyuwangi,blitar, bojonegoro,bondowoso,gresik,jember,kediri,lamongan,lumajang,madiun,magetan,malang,mojokerto,nganjuk,ngawi,pacitan,pamekasan,pasuruan,ponorogo,probolinggo,sampang,sidoarjo,situbondo,sumenep,trenggalek,tuban,tulungagung,kota batu,surabaya\n","['bangkalan', 'banyuwangi', 'blitar', ' bojonegoro', 'bondowoso', 'gresik', 'jember', 'kediri', 'lamongan', 'lumajang', 'madiun', 'magetan', 'malang', 'mojokerto', 'nganjuk', 'ngawi', 'pacitan', 'pamekasan', 'pasuruan', 'ponorogo', 'probolinggo', 'sampang', 'sidoarjo', 'situbondo', 'sumenep', 'trenggalek', 'tuban', 'tulungagung', 'kota batu', 'surabaya']\n","bangkalan\n","Processing city: bangkalan\n","8\n","No more page left. Scraping is done\n","banyuwangi\n","Processing city: banyuwangi\n","25\n","Processing city: banyuwangi\n","30\n","Processing city: banyuwangi\n","20\n","Processing city: banyuwangi\n","20\n","Processing city: banyuwangi\n","28\n","No more page left. Scraping is done\n","blitar\n","Processing city: blitar\n","30\n","Processing city: blitar\n","25\n","Processing city: blitar\n","20\n","Processing city: blitar\n","25\n","Processing city: blitar\n","25\n","Processing city: blitar\n","20\n","Processing city: blitar\n","30\n","Processing city: blitar\n","20\n","Processing city: blitar\n","20\n","Processing city: blitar\n","30\n","Processing city: blitar\n","20\n","Processing city: blitar\n","25\n","Processing city: blitar\n","30\n","Processing city: blitar\n","20\n","Processing city: blitar\n","30\n","No more page left. Scraping is done\n"," bojonegoro\n","Processing city:  bojonegoro\n","6\n","No more page left. Scraping is done\n","bondowoso\n","Processing city: bondowoso\n","18\n","No more page left. Scraping is done\n","gresik\n","Processing city: gresik\n","20\n","Processing city: gresik\n","25\n","Processing city: gresik\n","20\n","Processing city: gresik\n","25\n","Processing city: gresik\n","20\n","Processing city: gresik\n","25\n","Processing city: gresik\n","20\n","Processing city: gresik\n","30\n","Processing city: gresik\n","20\n","Processing city: gresik\n","25\n","Processing city: gresik\n","25\n","Processing city: gresik\n","20\n","Processing city: gresik\n","25\n","Processing city: gresik\n","25\n","Processing city: gresik\n","20\n","No more page left. Scraping is done\n","jember\n","Processing city: jember\n","30\n","Processing city: jember\n","24\n","Processing city: jember\n","4\n","No more page left. Scraping is done\n","kediri\n","Processing city: kediri\n","30\n","Processing city: kediri\n","20\n","Processing city: kediri\n","20\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","20\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","Processing city: kediri\n","25\n","No more page left. Scraping is done\n","lamongan\n","Processing city: lamongan\n","25\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EmWnRXr_HaiF"},"source":["bangkalan,banyuwangi,blitar,bojonegoro,bondowoso,gresik,jember,kediri,lamongan,lumajang,madiun,magetan,malang,mojokerto,nganjuk,ngawi,pacitan,pamekasan,pasuruan,ponorogo,probolinggo,sampang,sidoarjo,situbondo,sumenep,trenggalek,tuban,tulungagung,kota batu,surabaya"]}]}